{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Traffic Sign Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Project Writeup Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This Project Report contains the work I did for Udacity's Traffic sign classification project which is part of Self-driving Car Nano Degree that I am currently doing. This Report presents the code to train a convolutional network to identify German Traffic Signs. \n",
    "\n",
    "Before getting into detailed modeling below are some of the reference on which I have modeled Network Model\n",
    "\n",
    "#1. Hvass Laboratories Model of Convolutional Neural Network for recognizing hand-written digits in the MNIST data-set https://www.youtube.com/watch?v=HMcx-zY8JSg\n",
    "#2. Stanford's CS231n: Convolutional Neural Networks for Visual Recognition http://cs231n.stanford.edu/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The goals / steps of this project are the following:\n",
    "\n",
    "    * Load the data set of Images of German Traffic Sign DataSet provided by Udacity\n",
    "    * Explore, summarize and visualize the data set\n",
    "    * Design, train and test a model architecture\n",
    "    * Use the model to make predictions on new images downloaded from Internet\n",
    "    * Analyze the softmax probabilities of the new images\n",
    "    * Summarize the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Classification of German traffic signs is the project provided by Udacity Self-Driving Car Nanodegree program, however the dataset is publicly available here (http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "Upon loading of the data, I could see the below classfification of the data with each image already sized as \"32,32,3\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Number of training examples = 34799\n",
    "Number of Validation examples = 4410\n",
    "Number of testing examples = 12630\n",
    "\n",
    "Image data shape = (32, 32, 3)\n",
    "Number of classes = 43\n",
    "<class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data is in the form of pickle and consits of 3 differnet pickle file namely :\n",
    "\n",
    "    train.p' - Set of images for Training the Model\n",
    "    valid.p' - Set of Images to validate the Model\n",
    "    test.p'  - Set of Images to Test once the Model has been completly Trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Explore, summarize and visualize the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once the Data is loaded into the varaibles, analysed, we could see that the data is classified into total of 43 categories.\n",
    "i.e the data consits of 43 different traffic signs.\n",
    "\n",
    "Below I expolred the data in terms of the composition, number of samples per category and respective sign category and discription as per provided in reference doc - signnames.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below are the list of Labels, Discription and # of image sample in each one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Class 0: 0,Speed limit (20km/h) : 180 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 1: 1,Speed limit (30km/h) : 1980 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 2: 2,Speed limit (50km/h) : 2010 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 3: 3,Speed limit (60km/h) : 1260 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 4: 4,Speed limit (70km/h) : 1770 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 5: 5,Speed limit (80km/h) : 1650 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 6: 6,End of speed limit (80km/h) : 360 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 7: 7,Speed limit (100km/h) : 1290 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 8: 8,Speed limit (120km/h) : 1260 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 9: 9,No passing : 1320 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 10: 10,No passing for vehicles over 3.5 metric tons : 1800 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 11: 11,Right-of-way at the next intersection : 1170 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 12: 12,Priority road : 1890 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 13: 13,Yield : 1920 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 14: 14,Stop : 690 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 15: 15,No vehicles : 540 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 16: 16,Vehicles over 3.5 metric tons prohibited : 360 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 17: 17,No entry : 990 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 18: 18,General caution : 1080 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 19: 19,Dangerous curve to the left : 180 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 20: 20,Dangerous curve to the right : 300 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 21: 21,Double curve : 270 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 22: 22,Bumpy road : 330 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 23: 23,Slippery road : 450 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 24: 24,Road narrows on the right : 240 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 25: 25,Road work : 1350 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 26: 26,Traffic signals : 540 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 27: 27,Pedestrians : 210 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 28: 28,Children crossing : 480 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 29: 29,Bicycles crossing : 240 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 30: 30,Beware of ice/snow : 390 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 31: 31,Wild animals crossing : 690 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 32: 32,End of all speed and passing limits : 210 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 33: 33,Turn right ahead : 599 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 34: 34,Turn left ahead : 360 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 35: 35,Ahead only : 1080 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 36: 36,Go straight or right : 330 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 37: 37,Go straight or left : 180 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 38: 38,Keep right : 1860 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 39: 39,Keep left : 270 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 40: 40,Roundabout mandatory : 300 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 41: 41,End of no passing : 210 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "Class 42: 42,End of no passing by vehicles over 3.5 metric tons : 210 samples\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![alt text](image1.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "![alt text](image2.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Seeing the above histograph, we can see that the data is not evenly distributed. Some labels have very few samples.\n",
    "Also the data size for training has only 34799 sample. This is also very less when we want to train the neural network.\n",
    "\n",
    "Hence we will Augument the data.\n",
    "\n",
    "Also the randomly sampled images displayed, the images differ significantly in terms of contrast and brightness, so we will need to apply some kind of histogram equalization, this should noticeably improve feature extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Augmentation and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing:\n",
    "We will first apply brightness normalization to take away the effect of brightness variations, after applying brightness normalizations, the images transform as follows.\n",
    "\n",
    "Brightness normalized and transformed image\n",
    "We will rescale all the images so their intensity values vary between -.5 and .5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![alt text](image3.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Image Pre-Processing :\n",
    "\n",
    "# Applied the following preprocessing for image data,\n",
    "# first applied histogram equalization so the effect of brightness is removed. I used openCV'2 cv2\n",
    "# then scaled images between -.5 and .5, by dividing by 255. and subtracting .5.\n",
    "\n",
    "def pre_process_image(image):\n",
    "\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "    #image = image[:,:,0]\n",
    "    image[:,:,0] = cv2.equalizeHist(image[:,:,0])\n",
    "    image[:,:,1] = cv2.equalizeHist(image[:,:,1])\n",
    "    image[:,:,2] = cv2.equalizeHist(image[:,:,2])\n",
    "    image = image/255.-.5\n",
    "    #image = cv2.resize(image, (img_resize,img_resize),interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![alt text](image4.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data augmentation\n",
    "A big limitation of deep neural networks is that they may have millions of parameters, tuning which requires a vast data set. This however is not always possible. In such cases, data augmentation helps us generate additional training examples. I have generate additional data samples by applying affine transformation to the image. \n",
    "\n",
    "Affine transformations refer to transformations that do not alter the parallelism of lines, i.e. can be represented as a linear operation on the matrix. We will specifically use rotation, shearing and translation to simulate the effect of viewing the sign from different angles and different distances. Figure below presents original image and augmented images generated from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![alt text](image4.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_image(image,ang_range,shear_range,trans_range):\n",
    "\n",
    "    # Rotation\n",
    "\n",
    "    ang_rot = np.random.uniform(ang_range)-ang_range/2\n",
    "    rows,cols,ch = image.shape    \n",
    "    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)\n",
    "\n",
    "    # Translation\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "\n",
    "    # Shear\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "\n",
    "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "\n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "\n",
    "    image = cv2.warpAffine(image,Rot_M,(cols,rows))\n",
    "    image = cv2.warpAffine(image,Trans_M,(cols,rows))\n",
    "    image = cv2.warpAffine(image,shear_M,(cols,rows))\n",
    "\n",
    "    #image = pre_process_image(image)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Convulation Nueral Network Model that I decided is pretty much same one from the lab execrise with some modifications in order to get better results.\n",
    "\n",
    "It is fairly simple and has 4 layers: 3 convolutional layers for feature extraction and 3 fully connected layer as a classifier along with inital layer for 1x1x3 filters for chaning the color maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The project thoroughly discusses the approach taken for deriving and designing a model architecture fit for solving the problem given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Why did I select LeNet over other models --- Well I did go through lots of other models on the internet that various people/scientits and Various orginazations have created and implemented.\n",
    "\n",
    "Some of the models i look into are:\n",
    "\n",
    "[Spatial transformer network, 99.61% from Haloi]\n",
    "http://torch.ch/blog/2015/09/07/spatial_transformers.html\n",
    "https://github.com/Moodstocks/gtsrb.torch\n",
    "\n",
    "[Alex Staravoitau solution, 99.33% ]\n",
    "http://navoshta.com/traffic-signs-classification/\n",
    "\n",
    "[Vivek Yadav solution, 99.10%]\n",
    "https://medium.com/@vivek.yadav/improved-performance-of-deep-learning-neural-network-models-on-traffic-sign-classification-using-6355346da2dc#.a96x4gz1p\n",
    "https://chatbotslife.com/german-sign-classification-using-deep-learning-neural-networks-98-8-solution-d05656bf51ad#.pgsssrrdt\n",
    "\n",
    " Also Dense Conlolution as well.\n",
    " \n",
    " Well each one of them is excellently implemented and achives great accuracy. Initially I was very much tempted to use one of these model and try to reprouce if I can achive around same kind of Accuracy.\n",
    " \n",
    " But then decided to take the simple form of the model that I had already implemented in the lab and thought to check if I can work on the same LeNet Model and see if I can change some of the parameter and achive greater than 90% accuracy.\n",
    " \n",
    " When I implemented same LeNet, I was not able to achive it. Then I started expermenting with various values and then started changing the model a bit. \n",
    " \n",
    " Finally settled on the model with 4 layers: 3 convolutional layers for feature extraction and 3 fully connected layer as a classifier along with inital layer for 1x1x3 filters for chaning the color maps.\n",
    " \n",
    " With this I was able to achive 95% Accuracy on Validation and 93% Accuracy on the Testing set.\n",
    " \n",
    " I was farily satisfited with the model I built with some expermentation and hence I thought to stick with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In most applications, changing color map can result in significant improvements in performance. However, it is not clear what the best color map is for different applications, therefore using 3 1X1 filters results in a case were the network itself ends up choosing the best color map.\n",
    "\n",
    "The next 2 layer modules are composed of 32 and 64 (respectively) 3X3 filters followed by maxpooling and dropouts. The output from each of the convolution module is fed into a feedforward layer. Rationale being that the fully connected layer has access to outputs from low level and higher level filters and has the ability to choose the features that works the best. The feedfoward layers are composed of 2 hidden layers with filter size of 128 and 84 respectively, before being fed to O/P layer. Additional dropout layers are applied after each of the fully connected layers.\n",
    "The idea of using drop outs heavily is to avoid overfitting and force the network to learn multiple models for the same data.\n",
    "\n",
    "As opposed to usual strict feed-forward CNNs I use multi-scale features, which means that convolutional layers’ output is not only forwarded into subsequent layer, but is also branched off and fed into classifier (e.g. fully connected layer). Please note that these branched off layers undergo additional max-pooling, so that all convolutions are proportionally subsampled before going into classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I used the following regularization techniques to minimize overfitting to training data:\n",
    "\n",
    "### Dropout. \n",
    "Dropout is amazing and will drastically improve generalization of the model. Normally I only wanted to apply dropout to fully connected layers, as shared weights in convolutional layers are good regularizers themselves. However, I did notice a slight improvement in performance when using a bit of dropout on convolutional layers, thus left it in.\n",
    "\n",
    "### L2 Regularization. \n",
    "I ended up using lambda = 0.0001 which seemed to perform best. Important point here is that L2 loss should only include weights of the fully connected layers, and normally it doesn’t include bias term. Intuition behind it being that bias term is not contributing to overfitting, as it is not adding any new degree of freedom to a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A TensorFlow graph consists of the following parts which will be detailed below:\n",
    "\n",
    "    Placeholder variables used for inputting data to the graph.\n",
    "    Variables that are going to be optimized so as to make the convolutional network perform better.\n",
    "    The mathematical formulas for the convolutional network.\n",
    "    A cost measure that can be used to guide the optimization of the variables.\n",
    "    An optimization method which updates the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Placeholder variables serve as the input to the TensorFlow computational graph that I can change each time we execute the graph.\n",
    "First I define the placeholder variable for the input images. This allows me to change the images that are input to the TensorFlow graph. This is called tensor, which just means that it is a multi-dimensional vector or matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Each layer Filter Size and number of filters used:\n",
    "    \n",
    "    \n",
    "## layer 0 (Conv)\n",
    "filter_size0 = 1\n",
    "num_filters0 = 3\n",
    "\n",
    "## layer 1 (Conv)\n",
    "filter_size1 = 5\n",
    "num_filters1 = 32\n",
    "\n",
    "## layer 2 (Conv)\n",
    "filter_size2 = 5\n",
    "num_filters2 = 64\n",
    "\n",
    "## layer 3 (FC)\n",
    "## FC_size \n",
    "fc_size1 = 128\n",
    "\n",
    "## layer 4 (FC)\n",
    "## FC_size \n",
    "fc_size2 = 84\n",
    "\n",
    "## layer 5 (O/P)\n",
    "## FC_size \n",
    "fc_size3 = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Created the first convolutional layer. It takes x_image as input and creates 'num_filters1' different filters, each having width and height equal to filter_size of 'filter_size1'. Finally i down-sample the image so it is half the size by using 2x2 max-pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "      #### Layer 1 ######\n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    W_CL1 = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 32), mean = mu, stddev = sigma))\n",
    "    B_CL1 = tf.Variable(tf.zeros(32))\n",
    "    Conv_layer1 = tf.nn.conv2d(x, W_CL1, strides=[1, 1, 1, 1], padding='VALID') + B_CL1\n",
    "\n",
    "    # Activation for Layer1 (relu).\n",
    "    Conv_layer1 = tf.nn.relu(Conv_layer1) \n",
    "    #Drop-Out \n",
    "    Conv_layer1 = tf.nn.dropout(Conv_layer1, keep_prob)\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    Conv_layer1 = tf.nn.max_pool(Conv_layer1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The layer is activited by Relu followed by Drop-out and Max Pooling. This is same for all the Convolution Layers created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The convolutional layers output 4-dim tensors. Since now i want to use these as input in a fully-connected network, which requires for the tensors to be reshaped or flattened to 2-dim tensors.\n",
    "\n",
    "Additionally, as opposed to usual feed-forward CNNs I use multi-scale features, which means that convolutional layers’ output is not only forwarded into subsequent layer, but is also branched off and fed into classifier (e.g. fully connected layer). So here instead of just flattening out Layer2 o/p. I also flatten out layer1 o/p as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    " # Flatten. \n",
    "    Flat_out1 = flatten(Conv_layer1)\n",
    "    Flat_out2 = flatten(Con_layer2)\n",
    "    \n",
    "    Flat_out = tf.concat([Flat_out1, Flat_out2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Added a 2 fully-connected layer to the network. The input is the flattened layer from the previous convolution. The number of neurons or nodes in the fully-connected layer is fc_size. ReLU is used so we can learn non-linear relations along with drop-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "        #### Layer 3 ######\n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    W_FCL1 = tf.Variable(tf.truncated_normal(shape=(7872, 128), mean =mu, stddev = sigma))\n",
    "    B_FCL1 = tf.Variable(tf.zeros(128))\n",
    "    Fully_conn_layer1 = tf.matmul(Flat_out, W_FCL1) + B_FCL1\n",
    "    \n",
    "    # Activation.\n",
    "    Fully_conn_layer1 = tf.nn.relu(Fully_conn_layer1)\n",
    "    #Drop-Out \n",
    "    Fully_conn_layer1 = tf.nn.dropout(Fully_conn_layer1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "I first started with large augmentation so the model learns overall features of traffic sign, and we will gradually reduce the augmentation to fine tune the model. The training is carried out in the following steps,\n",
    "\n",
    "Preprocess all the training data\n",
    "Generate 10 new images per image in the training set using data augmentation\n",
    "25 itration with varoius values of drop-out\n",
    "0.001 as learning rate for optimization of weights\n",
    "\n",
    "\n",
    "rate = 0.001\n",
    "keep_prob = 0.7\n",
    "beta = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below defined Mathametical funtions are used to calculate the Logits and there by calcualte the Loss (Cost) and then this loss is optimized by using the AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logits, regularizers = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)*beta*regularizers\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "# Label predictions:\n",
    "y_pred = tf.nn.softmax(logits)\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Accuracy of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once the model is trainined, I used the below functions to calculate the accuracy on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        #cls_pred = sess.run(labels_pred_cls, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return (total_accuracy / num_examples)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predicted Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The final fully-connected layer estimates how likely it is that the input image belongs to each of the 43 classes. However, these estimates are a bit rough and difficult to interpret because the numbers may be very small or large, so we want to normalize them so that each element is limited between zero and one and the 43 elements sum to one. This is calculated using the called softmax function (which provides the probabilities) and the result is stored in y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "The class-number is the index of the largest element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, dimension=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Training and Accuracy calculaton on Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once all the helper-functions, Variables and constants defined, I created the Tensorflow instance and initititated all the Tensor flow varaibales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Calculate and report the accuracy on the training and validation set.\n",
    "### Once a final model architecture is selected, \n",
    "### the accuracy on the test set should be calculated and reported as well.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"# Training Samples:\",num_examples)\n",
    "    print()\n",
    "    print(\"Training....\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        #X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Training Samples: 347990\n",
    "\n",
    "Training....\n",
    "\n",
    "EPOCH 1 ...\n",
    "Validation Accuracy = 83.900\n",
    "\n",
    "EPOCH 2 ...\n",
    "Validation Accuracy = 85.646\n",
    "\n",
    "EPOCH 3 ...\n",
    "Validation Accuracy = 89.546\n",
    "\n",
    "EPOCH 4 ...\n",
    "Validation Accuracy = 91.995\n",
    "\n",
    "EPOCH 5 ...\n",
    "Validation Accuracy = 92.925\n",
    "\n",
    "EPOCH 6 ...\n",
    "Validation Accuracy = 94.739\n",
    "\n",
    "EPOCH 7 ...\n",
    "Validation Accuracy = 94.694\n",
    "\n",
    "EPOCH 8 ...\n",
    "Validation Accuracy = 95.646\n",
    "\n",
    "EPOCH 9 ...\n",
    "Validation Accuracy = 95.986\n",
    "\n",
    "EPOCH 10 ...\n",
    "Validation Accuracy = 95.601\n",
    "\n",
    "EPOCH 11 ...\n",
    "Validation Accuracy = 94.717\n",
    "\n",
    "EPOCH 12 ...\n",
    "Validation Accuracy = 94.580\n",
    "\n",
    "EPOCH 13 ...\n",
    "Validation Accuracy = 95.351\n",
    "\n",
    "EPOCH 14 ...\n",
    "Validation Accuracy = 94.717\n",
    "\n",
    "EPOCH 15 ...\n",
    "Validation Accuracy = 94.739\n",
    "\n",
    "EPOCH 16 ...\n",
    "Validation Accuracy = 94.218\n",
    "\n",
    "EPOCH 17 ...\n",
    "Validation Accuracy = 95.034\n",
    "\n",
    "EPOCH 18 ...\n",
    "Validation Accuracy = 95.442\n",
    "\n",
    "EPOCH 19 ...\n",
    "Validation Accuracy = 95.465\n",
    "\n",
    "EPOCH 20 ...\n",
    "Validation Accuracy = 96.531\n",
    "\n",
    "Model saved\n",
    "Time usage: 0:25:02\n",
    "In [18]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    \n",
    "    # Predict the New Image Classification:\n",
    "    prediction1 = tf.argmax(logits, 1)\n",
    "    img1 = X_train[10:20] # At this time testing with one of the Train Image itself\n",
    "   \n",
    "    for i in range(10):\n",
    "        plt.imshow(img1[i])\n",
    "        plt.show()\n",
    "        class1 = sess.run(prediction1, feed_dict={x: [img1[i]]})\n",
    "        class_int = int(class1)\n",
    "        #print(\"class1\",class_int)\n",
    "        csv_name = sign_name[class_int + 1]\n",
    "        print(\"CLASS-ID:{0},: Dis:{1}\".format(class1, csv_name))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Test Accuracy = 93.959\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test a Model on New Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once the Accuracy on the Testing set was good, I procceded to test the model on the images downloaded from the internet.\n",
    "\n",
    "Below Helper function is used to preprocess the image to resize the shape to 32,32,3 and grayscale and normalize the images and display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_newimage_file(name):\n",
    "    image = cv2.imread(name)\n",
    "    image = cv2.resize(image,(32,32))\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image = image/255.-.5\n",
    "    return image\n",
    "\n",
    "def plot_newImage_3C(n_row,n_col,X,namenewdata):\n",
    "\n",
    "    plt.figure(figsize = (8,6))\n",
    "    gs1 = gridspec.GridSpec(n_row,n_row)\n",
    "    gs1.update(wspace=0.01, hspace=0.15) # set the spacing between axes. \n",
    "\n",
    "    for i in range(n_row*n_col):\n",
    "        # i = i + 1 # grid spec indexes from 0\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        plt.axis('on')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.set_aspect('equal')\n",
    "        #plt.subplot(4,11,i+1)\n",
    "        ind_plot = i\n",
    "        plt.imshow(X[ind_plot])\n",
    "        #plt.title(namenewdata[ind_plot].split('.'))\n",
    "        plt.title(namenewdata[ind_plot].split('.')[0])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The images are stored in \"/new_images\" directory and read into the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "import os\n",
    "\n",
    "newdata = [process_newimage_file(\"./new_signs/\"+name) for name in os.listdir(\"./new_signs/\")]\n",
    "namenewdata = [name for name in os.listdir(\"./new_signs/\")]\n",
    "newdata = np.array(newdata ,dtype = np.float32)\n",
    "print(type(newdata))\n",
    "print(len(newdata))\n",
    "print(namenewdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<class 'numpy.ndarray'>\n",
    "9\n",
    "['StopSign.jpg', 'TrafficSign.jpg', 'No-Entry.jpg', '30-miles.jpg', 'Curve-Ahead.jpg', 'MenAtWork.jpg', 'Left-Turn.jpg', 'Bump.jpg', 'Yield.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Visualization of New Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The New Images were randomly searched on google for German Traffic Signs. I tried to select as much different category of images as possible so that the model is tested throughly.\n",
    "\n",
    "I also tried to select images :\n",
    "    - That had very less samples in the training set like Stop, No-entry to see if Model still classify them correctly\n",
    "    - Images that had background images like cloud, building, trees to see if model can still detect the signs\n",
    "    - Variuos brightness\n",
    "    - slightly unknown images that is not present in the orginal training set (Atleast when randomly displayed, I did not see the image like \"Curvey road\" image that i have downloaded)\n",
    "    \n",
    "All the selected New images are being displayed in the .ipynb file before feeding into the system to review the type and quality of the each i/p images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below tensor function is used to predict the new image class by getting the softmax probabilites of the image from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here and use the model to output the prediction for each image.\n",
    "### Make sure to pre-process the images with the same pre-processing pipeline used earlier.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "keep_prob = 1.0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    \n",
    "    \n",
    "    prediction1 = tf.argmax(logits, 1)\n",
    "    class1 = sess.run(prediction1, feed_dict={x: newdata})\n",
    "    \n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    result = sess.run(softmax, feed_dict={x: newdata})\n",
    "    values, indices = tf.nn.top_k(result, 5)\n",
    "    probs = sess.run(values)\n",
    "    predictions = sess.run(indices)\n",
    "    top5_pred = predictions\n",
    "    #print(\"values\",values)\n",
    "    #print(\"indices\",indices)\n",
    "    \n",
    "    #print(\"org-probs\",probs)\n",
    "    #print(\"predictions\",predictions)\n",
    "    print(\"Predicition Complete.\")\n",
    "    print(\"Predicited Class ID\",class1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Predicition Complete.\n",
    "Predicited Class ID [13 31 17  2 25 25 19 22 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Model Predicted 7 out 9 images correctly.\n",
    "\n",
    "2 incorrectly identified images are \n",
    "    - 30 kmph\n",
    "    - Curve road ahead\n",
    " \n",
    "Observations:\n",
    "I could observe that numbers are incorrectly being detected in the model.I tried with bunch of speed limit images to see if it was only with 30 or any other numbers. I tried with 80, 70, 20 and 100.\n",
    "All these were in-correctly detected randomly. When tried multiple times, it did detect them correctly though.\n",
    "\n",
    "Even the 30kmph that is finally detected as 20 was also dtected correctly during initial testing.\n",
    "\n",
    "The other observation i could make was, there is a slight difference b/w the o/p of logits before softmax and after softmax equations.\n",
    "\n",
    "    prediction1 = tf.argmax(logits, 1)\n",
    "     softmax = tf.nn.softmax(logits)\n",
    "     \n",
    "For Example below is the o/p from both the above mentioned functions.\n",
    "\n",
    "From Predicition1 -- >Predicited Class ID  [13 31 17  2 25 25 19 22 13]\n",
    "From Softmax and Top K ----> Predicited ID: 14,26,17, 0,26,25,19,22,13\n",
    "\n",
    "The last 4 images and 3rd image matched with both the equations. \n",
    "\n",
    "However, the model did predict the low training sample images (like no-entry, stop signs)correctly which I was happy about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The predicted images are analyzed by further calculating the top_5 label values from Softmax. This is ploted in the graph to show how confident is the system in predicting the new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Calculate the accuracy for these 5 new images. \n",
    "### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images.\n",
    "\n",
    "from pylab import *\n",
    "\n",
    "for i in range(9):\n",
    "    performance = [float(\"{:.2f}\".format(x)) for x in probs[i]]\n",
    "    plt.figure(figsize = (5,1.5))\n",
    "    gs = gridspec.GridSpec(1, 2,width_ratios=[2,3])\n",
    "    plt.subplot(gs[0])\n",
    "    plt.imshow(newdata[i]+.5)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(gs[1])\n",
    "    plt.barh(7-np.arange(5),performance, align='center')\n",
    "    for i_label in range(5):\n",
    "        plt.text(probs[i][i_label]+.02,6-i_label-.25,sign_name[top5_pred[i][i_label]+1])\n",
    "    plt.axis('off');\n",
    "    plt.text(0,8.0,namenewdata[i].split('.')[0]);\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Out of 9 Images, the model predicted 7 images correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Please note all the images and data is loaded in the .ipynb file. I am unable to load the same images in this report.\n",
    "Please let me know if any changes needs to be done or if any ways I can improve this more.\n",
    "\n",
    "I wanted to also use tensor Board to analyse the process happening inside the tensorflow. However yet to exlore the tensor borad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
